<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="files/jemdoc.css" type="text/css" />

<title>Jinlong Li</title>

</head>
<body>

<!-- Project
<div class="menu"> <a href="#home">Home</a> 
<a href="#publications">Publications</a> 
<a href="#services">Services</a> 
<a href="#awards">Awards</a>  
</div>
 -->

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 


<table class="imgtable"><tr><td>
<a href="./"><img src="./files/photo.jpeg" alt="" height="250px" /></a>&nbsp;</td>
<td align="left"><p><font size="5">Jinlong Li (李金龙)</font><br />
<br />
<a>First-year PhD Student.</a><br />
Multimedia and  Human Understanding Group, <a href="http://mhug.disi.unitn.it/" target="_blank">MHUG</a><br />
<br />
<!-- <a href="https://about.meituan.com/">Meituan Inc.</a><br />
 Vision Department, <a target="_blank">Research Scientist</a><br />
<br />
District Longhua, Shenzhen, Guangdong, China<br /> -->

<br />
Email: jinlong.szu@gmail.com <br />
<!-- [<a class="p1" href="https://scholar.google.com/citations?user=hpEAymEAAAAJ&hl=zh-CN&authuser=1" target="_blank">Google Scholar</a>]  -->
<!-- [<a class="p2" href="https://github.com/TyroneLi" target="_blank">Github</a>] -->
<p>
  <a href="https://github.com/TyroneLi"><img src="./index_files/github.png" height="30px"></a>
  <a href="https://scholar.google.com/citations?user=hpEAymEAAAAJ&hl=zh-CN&authuser=1"><img src="./index_files/google_scholar.png" height="30px"></a>
</p>
</p>

<br /> <code> Pushing a long-termist, strike the tough yet right things. </code> <br />

</td></tr></table>

<h2>Biography</h2>
<p> 
  I am now a first-year PhD student at the Multimedia and  Human Understanding Group (<a href="http://mhug.disi.unitn.it/">MHUG</a>), 
  under the supervision of <a href="https://disi.unitn.it/~sebe/">Prof. Nicu Sebe</a>.
  Previously, I worked at <a href="https://about.meituan.com/">MeiTuan</a> as a computer vision scientist, working closely with <a href="https://forestlinma.com/">Dr. Lin Ma</a> 
  and <a href="https://scholar.google.com/citations?user=4sKGNB0AAAAJ&hl=zh-CN">Dr. Zequn Jie</a>, focusing on 2D/3D Label-Efficient Detection and Segmentation. 
  Before that, I obtained my B.Sc and M.Sc degree in <a href="https://en.szu.edu.cn/">Shenzhen University</a>. 
</p>


<h2>Research Interest</h2>
I work in the field of Computer Vision and Deep Learning. Recently, I focus on the following research topics:
<ul>
<li>Label-Efficient Learning on both 2D and 3D</li>
<li>Multi-Modal Perception and Reasoning Learning</li>
<li>Generative and Foundation Models</li>
<!-- <li>Foundation Models</li> -->
</ul>

<!-- I am actively looking forward to collaborate with people who also shares similar research interests and passions wth me, please do not hesitate to drop me emails. -->


	
<!-- <h2>PyTorch Toolbox for Image Restoration</h2>
<ul>
<li> <a href="https://github.com/cszn/KAIR" target="_blank">KAIR</a> (support training and testing for DnCNN, FFDNet, SRMD, USRNet, ESRGAN) </li>
<li> <a href="https://github.com/cszn/DPIR" target="_blank">DPIR</a> (Plug-and-Play Image Restoration with Deep Denoiser Prior)</li>
</ul> -->


<h2>News</h2>
<ul>
  
  <li>
    2024 Mar.: Served as a reviewer for <a href="https://eccv2024.ecva.net/Conferences/2024/">ECCV 2023</a>.
  </li>

  <li>
    2024 Feb.: Served as a reviewer for <a href="https://openreview.net/group?id=ICML.cc/2024/Conference/">ICML 2024</a>.
  </li>

  <li>
    2023 Aug.: Served as a reviewer for <a href="https://openreview.net/group?id=ICLR.cc/2024/Conference/">ICLR 2024</a>.
  </li>

  <li>
    2023 Mar.: Served as a reviewer for <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Conference/">NeurIPS 2023</a>.
  </li>

  <li>
    2023 Feb.: One paper got accepted by <a href="https://www.editorialmanager.com/neucom/">Neurocomputing 2023</a>!
  </li>

  <li>
    2023 Feb.: Served as a reviewer for <a href="https://openreview.net/group?id=ICML.cc/2023/Conference/">ICML 2023</a>.
  </li>

  <li>
    2023 Jan.: Served as a reviewer for <a href="https://openreview.net/group?id=thecvf.com/CVPR/2023/Conference/">CVPR 2023</a>.
  </li>

  <li>
    2022 Sept.: Our work <a href="http://arxiv.org/abs/2209.07761/">ESOL</a> got accepted by <a href="https://nips.cc/">NeurIPS 2022</a>!
  </li>

  <li>
    2022 Feb.: Our work <a href="https://ieeexplore.ieee.org/document/9716799">PPL</a> got accepted by <a href="https://ieeexplore.ieee.org/document/9716799">TMM 2022</a>!
  </li>

</ul>

	
<!-- Project -->
<a id="publications" class="anchor"></a>
<h2>Publications</h2>

<table class="imgtable">

<!-- NEUCOM 2022-->
<tr>
<td><img class="proj_thumb" src="./files/NEURALCOM_2023_SSDL.png" alt="" height="120px"/>&nbsp;</td>
<td>
<p class="pub_title">Weakly Supervised Semantic Segmentation via Self-Supervised Destruction Learning</p>
<p class="pub_author"><b>Jinlong Li</b>, Zequn Jie, Xu Wang, Yu Zhou, Lin Ma, Jianming Jiang<br>
  Neurocomputing</i> (<b>NEUCOM</b>), 2023.<br>
  <p style="text-align:justify; text-justify:inter-ideograph;color:red;">
    Abstract: In this paper, we propose a novel “destruction learning” method via self-supervised manner,
    producing the CAM attention maps better covering the whole object rather than only the most discriminative regions
    as previous approaches. Region destruction mechanism is proposed to deliberately “destruct” the global structure in
    both mid-level and low-level feature learning following jigsaw puzzle operation, for better local feature extraction of
    the classification network.
  </p>
</p> </td>
</tr>

<!-- NeurIPS 2022-->
<tr>
  <td><img class="proj_thumb" src="./files/esol_intro1.png" alt="" height="120px"/>&nbsp;</td>
  <td>
  <p class="pub_title">Expansion and Shrinkage of Localization for Weakly-Supervised Semantic Segmentation</p>
  <p class="pub_author"><b>Jinlong Li</b>, Zequn Jie, Xu Wang, Xiaolin Wei, Lin Ma<br>
    Neural Information Processing Systems (<b>NeurIPS</b>), </em> <b>Spotlight (1.7%)</b> 2022.<br>
  [<a href="http://arxiv.org/abs/2209.07761/">ArXiv Link</a>]
  [<a href="https://github.com/TyroneLi/ESOL_WSSS">Code</a>]<br>
  <!-- [<a href="bibtex.html#zhang2022practical" target="_blank">BibTex</a>] -->
  <p style="text-align:justify; text-justify:inter-ideograph;color:red;">
    Abstract: We propose a new training pipeline to alleviate the partial localization 
    issue of the CAM in Weakly-supervised image semantic segmentation, ESOL, in a Divide-and-Conquer manner.
  </p>
  </p> </td>
  </tr>

<!-- TMM 2022-->
<tr>
  <td><img class="proj_thumb" src="./files/ppl_intro1.png" alt="" height="120px"/>&nbsp;</td>
  <td>
  <p class="pub_title">Weakly Supervised Semantic Segmentation via Progressive Patch Learning</p>
  <p class="pub_author"><b>Jinlong Li</b>, Zequn Jie, Xu Wang, Yu Zhou, Xiaolin Wei, Lin Ma<br>
    IEEE Transactions on Multimedia (<b>TMM</b>), </em>2022.<br>
  [<a href="https://arxiv.org/abs/2209.07828/">ArXiv Link</a>]
  [<a href="https://ieeexplore.ieee.org/document/9716799/">IEEE Trans Link</a>]
  [<a href="https://github.com/TyroneLi/PPL_WSSS">Code</a>]<br>
  <!-- [<a href="bibtex.html#zhang2022practical" target="_blank">BibTex</a>] -->
  <p style="text-align:justify; text-justify:inter-ideograph;color:red;">
    Abstract: We propose a new training pipeline to alleviate the partial localization issue of 
    the CAM in Weakly-supervised image semantic segmentation, PPL, in an iterative training manner.
  </p>
  </p> </td>
  </tr>

<table>

<!-- Experience -->
<a id="experience" class="anchor"></a>
<h2>Experience</h2>
<font size="3">
<ul>
<!-- <li><p>Computer Vision Engineer, MeiTuan Inc., 2022.07 - Present </p></li>
<li><p>Intern, MeiTuan Inc., 2020.12 - 2022.04 </p></li>
<li><p>Image Algorithm Engineer, Reetoo Biotech, 2017.07 - 2019.04 </p></li> -->
<li>
  <div style="float:left; text-align:left">Research Scientist, MeiTuan Inc.</div> <div style="text-align:center"><p style="text-align: right;">2022.07 - 2023.10 &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;</p></div>
</li>

<!-- <li>
  <div style="float:left; text-align:left">Intern, MeiTuan Inc.</div> <div style="float:center; text-align:center">&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; 2020.12 - 2022.04</div>
</li> -->

<li>
  <div style="float:left; text-align:left">Intern, MeiTuan Inc.</div> <div style="text-align:center"><p style="text-align: right;">2020.12 - 2022.04 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</p></div>
</li>

<li>
  <div style="float:left; text-align:left">Image Algorithm Engineer, Reetoo Biotech.</div> <div style="text-align:center"><p style="text-align: right;">2017.07 - 2019.04 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</p></div>
</li>

</ul>
</font>	


<!-- Services -->
<a id="services" class="anchor"></a>
<h2>Services</h2>

<!-- <p>Workshop Organizers: </p>
<font size="2">
<ul>
<li><p>Co-organizer of ECCV 2020 Workshop on Advanced Image Manipulation (AIM). </p></li>
<li><p>Co-organizer of CVPR 2020 Workshop on New Trends in Image Restoration and Enhancement (NTIRE). </p></li>
<li><p>Co-organizer of ICCV 2019 Workshop on Advanced Image Manipulation (AIM). </p></li>
</ul>
</font> -->

 
<p>Journal Reviewer:  </p>
<font size="3"> 
<ul>
<li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
<!-- <li>International Journal of Computer Vision (IJCV)</li> -->
<!-- <li>IEEE Transactions on Image Processing (TIP)</li> -->
<!-- <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li> -->
<li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
<!-- <li>Computer Vision and Image Understanding (CVIU)</li> -->
<!-- <li>Signal Processing Letters (SPL)</li> -->
</ul>
</font>


<p>Conference Reviewer: </p>
<font size="3"> 
<ul>
  <li>International Conference on Learning Representations (ICLR)</li>
  <li>Neural Information Processing Systems (NeurIPS)</li>
  <li>International Conference on Machine Learning (ICML)</li>
  <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
  <li>International Conference on Computer Vision (ICCV)</li>
  <li>European Conference on Computer Vision (ECCV)</li>
</ul>
</font>


<!-- awards -->
<a id="awards" class="achor"></a>
<h2>Awards</h2>
<font size="3"> 
<ul>
<li>Outstanding student paper award of SZU, 2023</li>
</ul>
</font>


<!-- Links 
<h2>Music</h2>
<iframe width="280" height="157" src="https://www.youtube.com/embed/-5qhNRmMilI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>	
<iframe width="280" height="157" src="https://www.youtube.com/embed/AVXejOoPECA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="280" height="157" src="https://www.youtube.com/embed/xTRVZbHjmbc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>	
-->
	
<!-- <div align="center">
	<body> 
		<font color="gray">&copy Jinlong Li</font>
	</body> 
</div> -->

	
<div id="footer">
<div id="footer-text">
<!--
All Rights Reserved. Part of page is generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
-->

</div>
</div>
<div align=""center>
<!-- <a href="https://clustrmaps.com/site/1b743"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=7HOnPG-tgP2NBIq9v142wI5iM0mQ3OwnnIRnYxx5SdI&cl=ffffff" width=1pt height=1pt/></a> -->
  <a href="https://info.flagcounter.com/xnxJ"><img src="https://s11.flagcounter.com/mini/xnxJ/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>
</div>

<div align="center">
	<body> 
		<font color="gray">&copy Jinlong Li</font>
	</body> 
</div>

</body>
</html>
